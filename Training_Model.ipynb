{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY2zBjvESekW/ggwVezOu8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kartikgc9/Smart-India-Hackathon-23/blob/main/Training_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2t-uM4b3DDd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pyts.image import GramianAngularField\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import layers\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the data\n",
        "cloudburst_data = np.load('CB6.npy')\n",
        "non_cloudburst_data = np.load('NB.npy')\n",
        "\n",
        "print(cloudburst_data.shape)\n",
        "print(non_cloudburst_data.shape)\n",
        "\n",
        "# Combine the cloudburst and non-cloudburst data into one array\n",
        "X = np.concatenate((cloudburst_data, non_cloudburst_data))\n",
        "\n",
        "# Create a target vector (1 for cloudburst, 0 for non-cloudburst)\n",
        "y = np.concatenate((np.ones(cloudburst_data.shape[0]), np.zeros(non_cloudburst_data.shape[0])))\n",
        "\n",
        "# Shuffle the data\n",
        "shuffle_index = np.random.permutation(X.shape[0])\n",
        "X = X[shuffle_index]\n",
        "y = y[shuffle_index]\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "BSfcTytx3HZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizers = ['sgd']\n",
        "kernel_sizes = [(1, 1), (2, 2), (3, 3)]\n",
        "pooling_sizes = [(1, 1), (2, 2), (3, 3)]\n",
        "learning_rates = [0.0001, 0.0005, 0.001]\n",
        "\n",
        "for i, opt in enumerate(optimizers):\n",
        "    for j, k_size in enumerate(kernel_sizes):\n",
        "        for l, p_size in enumerate(pooling_sizes):\n",
        "            for m, lr in enumerate(learning_rates):\n",
        "                # Get the current TensorFlow session\n",
        "                sess = tf.compat.v1.get_default_session()\n",
        "\n",
        "                # Close the session to free up the GPU memory\n",
        "                if sess is not None:\n",
        "                    sess.close()\n",
        "\n",
        "                # Reset the TensorFlow graph\n",
        "                tf.compat.v1.reset_default_graph()\n",
        "\n",
        "\n",
        "                try:\n",
        "                    # Create a new model for each hyperparameter combination\n",
        "                    model = Sequential(name=f'sequential_{i}_{j}_{l}_{m}')\n",
        "                    model.add(InputLayer(input_shape=(6, 256, 256)))\n",
        "                    model.add(Conv2D(filters=128, kernel_size=k_size, activation='relu', padding='same'))\n",
        "                    model.add(MaxPooling2D(pool_size=p_size, padding='same'))\n",
        "                    model.add(Conv2D(filters=256, kernel_size=k_size, activation='relu', padding='same'))\n",
        "                    model.add(MaxPooling2D(pool_size=(1, 1), padding='same'))\n",
        "                    model.add(Conv2D(filters=512, kernel_size=k_size, activation='relu', padding='same'))\n",
        "                    model.add(MaxPooling2D(pool_size=p_size, padding='same'))\n",
        "                    model.add(Flatten())\n",
        "                    model.add(Dense(units=256, activation='relu', kernel_regularizer='l1_l2'))\n",
        "                    model.add(Dropout(rate=0.5))\n",
        "                    model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "                    optimizer = None\n",
        "                    if opt == 'adam':\n",
        "                        optimizer = Adam(learning_rate=lr)\n",
        "                    elif opt == 'sgd':\n",
        "                        optimizer = SGD(learning_rate=lr)\n",
        "                    elif opt == 'rmsprop':\n",
        "                        optimizer = RMSprop(learning_rate=lr)\n",
        "\n",
        "                    # Compile the model with binary cross-entropy loss function\n",
        "                    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "                    # Train the model on your data with 50 epochs and batch size of 1\n",
        "                    history = model.fit(X, y, epochs=50, batch_size=1, validation_split=0.3)\n",
        "\n",
        "                    # Print the output of each combination\n",
        "                    print(f\"Optimizer: {opt}, Kernel Size: {k_size}, Pooling Size: {p_size}, Learning Rate: {lr}\")\n",
        "                    print(f\"Training accuracy: {history.history['accuracy'][-1]}, Validation accuracy: {history.history['val_accuracy'][-1]}\")\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "                    # Calculate and print the confusion matrix\n",
        "                    y_pred = np.round(model.predict(X)).flatten()\n",
        "                    y_true = y.flatten()\n",
        "                    cm = confusion_matrix(y_true, y_pred)\n",
        "                    print(\"Confusion Matrix:\")\n",
        "                    print(cm)\n",
        "\n",
        "                    accuracy = (cm[0,0]+cm[1,1])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])\n",
        "                    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "                    test = np.load('D:\\\\TEST12.npy')\n",
        "                    print(test.shape)\n",
        "                    prediction = model.predict(test)\n",
        "\n",
        "                    # Multiply the output by 100 and convert it to integers\n",
        "                    prediction_int = (prediction * 100).astype(int)\n",
        "\n",
        "                    # Print the result\n",
        "                    print(prediction_int)\n",
        "\n",
        "                    model.save(f'D:\\\\Models\\\\sequential_{i}_{j}_{l}_{m}.h5')\n",
        "\n",
        "                except Exception as e:\n",
        "                    # If there's an error, print a message and continue to the next combination\n",
        "                    print(f\"Error for hyperparameter combination optimizer={opt}, kernel_size={k_size}, pooling_size={p_size}, learning_rate={lr}: {e}\")\n",
        "                    continue"
      ],
      "metadata": {
        "id": "Hjr_XRXS3KAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}